{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eden1114/colab/blob/main/examples/anakin_ppo_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYFqK9lLJTjs"
      },
      "source": [
        "# Distributed PPO Anakin Agent using `flashbax` and `Jumanji`\n",
        "### [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/instadeepai/flashbax/blob/main/examples/anakin_ppo_example.ipynb)\n",
        "\n",
        "Adapted from [Gymnax Example](https://colab.research.google.com/github/RobertTLange/gymnax/blob/main/examples/01_anakin.ipynb) and DeepMind's [Example Colab](https://colab.research.google.com/drive/1974D-qP17fd5mLxy6QZv-ic4yxlPJp-G?usp=sharing#scrollTo=lhnJkrYLOvcs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBByAMGIJTjs"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GAD-q7Y0JTjs",
        "outputId": "f10b95a4-a479-4793-c215-f6530af92e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'flashbax'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c4ab039effde>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mflashbax\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfbx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flashbax'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import flashbax as fbx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gcfrYd1XJTjs",
        "outputId": "2a3cde05-03a4-4c71-93b4-74df755952af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'haiku'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-176b92a3390f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhaiku\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrlax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'haiku'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import jax\n",
        "from jax import lax\n",
        "from jax import random\n",
        "from jax import numpy as jnp\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "import optax\n",
        "import rlax\n",
        "import timeit\n",
        "import distrax\n",
        "import chex\n",
        "from jumanji.wrappers import AutoResetWrapper\n",
        "import jumanji\n",
        "\n",
        "print(\"The following devices are available: \", jax.devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6emDIN_sJTjs"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/instadeepai/flashbax.git\n",
        "!pip install ./flashbax[examples]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv45yzf1JTjs"
      },
      "source": [
        "### Anakin PPO Distributed Agent Setup\n",
        "The following is a simple PPO implementation in the Anakin framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JwcJV7yJTjt"
      },
      "outputs": [],
      "source": [
        "@chex.dataclass(frozen=True)\n",
        "class TimeStep:\n",
        "    observation: chex.Array\n",
        "    action: chex.Array\n",
        "    discount: chex.Array\n",
        "    reward: chex.Array\n",
        "    behaviour_action_log_prob: chex.Array\n",
        "    behaviour_value: chex.Array\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class Params:\n",
        "    online : hk.Params\n",
        "    update_count : int\n",
        "\n",
        "def get_network_fn(num_outputs: int):\n",
        "\n",
        "    def network_fn(obs: chex.Array) -> chex.Array:\n",
        "        x = hk.Sequential([\n",
        "            hk.Conv2D(32, kernel_shape=2, stride=1),\n",
        "            jax.nn.relu,\n",
        "            hk.Conv2D(32, kernel_shape=2, stride=1),\n",
        "            jax.nn.relu,\n",
        "            hk.Flatten(),\n",
        "            hk.Linear(256),\n",
        "            jax.nn.relu])(obs)\n",
        "\n",
        "        logits = hk.Sequential([\n",
        "            hk.Flatten(),\n",
        "            hk.Linear(128),\n",
        "            jax.nn.relu,\n",
        "            hk.Linear(num_outputs)])(x)\n",
        "\n",
        "        value = hk.Sequential([\n",
        "            hk.Flatten(),\n",
        "            hk.Linear(128),\n",
        "            jax.nn.relu,\n",
        "            hk.Linear(1)])(x)\n",
        "\n",
        "        return logits, value\n",
        "\n",
        "    return hk.without_apply_rng(hk.transform(network_fn))\n",
        "\n",
        "def get_learner_fn(\n",
        "    env, forward_pass, buffer_fn, opt_update, rollout_len, agent_discount, iterations, gae_lambda, clip_epsilon, sgd_epochs):\n",
        "    \"\"\"Returns a learner function that can be used to train the parameters of the agent.\"\"\"\n",
        "\n",
        "    def rollout(params, outer_rng, env_state, env_timestep):\n",
        "        \"\"\"Collects a single trajectory from the environment.\"\"\"\n",
        "\n",
        "        def step_fn(env_data, rng):\n",
        "            \"\"\"A single step of the environment.\"\"\"\n",
        "            env_state, env_timestep, params = env_data\n",
        "            obs_tm1 = env_timestep.observation.grid\n",
        "            d_tm1 = env_timestep.discount\n",
        "            a_logits_tm1, v_tm1 = forward_pass(params, jnp.expand_dims(obs_tm1, 0))\n",
        "            a_tm1_dist = distrax.Categorical(a_logits_tm1[0])\n",
        "            a_tm1 = a_tm1_dist.sample(seed=rng)\n",
        "            a_tm1_log_prob = a_tm1_dist.log_prob(a_tm1)\n",
        "            new_env_state, new_env_timestep = env.step(env_state, a_tm1)\n",
        "            r_t = new_env_timestep.reward\n",
        "\n",
        "            return (new_env_state, new_env_timestep, params), TimeStep(\n",
        "              observation = obs_tm1, action=a_tm1, discount=d_tm1, reward=r_t, behaviour_action_log_prob=a_tm1_log_prob, behaviour_value=v_tm1.squeeze())\n",
        "              # We line up the observation with its discount, not the discount of the next observation as is usually seen.\n",
        "              # This is so that we know in a transition that discount[1] is the discount of the next observation.\n",
        "              # e.g. indexing is v[t] = reward[t] + discount[t+1]*value[t+1]\n",
        "              # Switching to Sutton and Barto's notation, we would have v[t] = reward[t+1] + discount[t+1]*value[t+1]\n",
        "              # To do this, we would add r_tm1 = env_timestep.reward to the TimeStep dataclass, not the new_env_timestep.reward\n",
        "\n",
        "        step_rngs = random.split(outer_rng, rollout_len)\n",
        "        (env_state, env_timestep, params), rollout = lax.scan(step_fn, (env_state, env_timestep, params), step_rngs)\n",
        "\n",
        "        rollout = jax.tree.map(lambda x: jnp.expand_dims(x,0), rollout)\n",
        "\n",
        "        return rollout, env_state, env_timestep\n",
        "\n",
        "    def loss_fn(params : hk.Params, batch : TimeStep):\n",
        "        \"\"\"Computes the loss of the agent on a single trajectory.\"\"\"\n",
        "\n",
        "        logits_t, v_t = forward_pass(params, batch.observation)\n",
        "        a_log_prob_t = distrax.Categorical(logits_t).log_prob(batch.action.astype(jnp.int32))\n",
        "        adv_t = rlax.truncated_generalized_advantage_estimation(batch.reward[:-1], batch.discount[1:] * agent_discount, gae_lambda, batch.behaviour_value, True)\n",
        "        rhos = jnp.exp(a_log_prob_t[:-1] - batch.behaviour_action_log_prob[:-1])\n",
        "        pg_loss = rlax.clipped_surrogate_pg_loss(rhos, adv_t, clip_epsilon)\n",
        "        entropy_loss = distrax.Categorical(logits_t).entropy()[:-1]\n",
        "        target_values = batch.behaviour_value[:-1] + adv_t\n",
        "        value_loss = jnp.square(v_t[:-1] - target_values)\n",
        "        loss = pg_loss + value_loss - 0.001*entropy_loss\n",
        "        loss = jnp.mean(loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def update_fn(params_state : Params, buffer_state, opt_state, rng, env_state, env_timestep):\n",
        "        \"\"\"Performs a rollout and updates the parameters of the agent.\"\"\"\n",
        "\n",
        "        rng, rollout_rng = random.split(rng)\n",
        "\n",
        "        data_rollout, new_env_state, new_env_timestep = rollout(params_state.online, rollout_rng, env_state, env_timestep)  # collect trajectory.\n",
        "        # The use of a queue here is not necessary since we push data in and immediately pop it out, but it is an example of how one could use it.\n",
        "        # Note: we do not check can_add and can_sample here, but we could use jax.lax.cond to do so.\n",
        "        # Branching slows down training and since we know for a fact that we can add and sample, we do not need to check.\n",
        "        buffer_state = buffer_fn.add(buffer_state, data_rollout)\n",
        "        buffer_state, batch = buffer_fn.sample(buffer_state)\n",
        "        # We get rid of the batch dimension here\n",
        "        batch = jax.tree.map(lambda x: jnp.squeeze(x, 0), batch.experience)\n",
        "\n",
        "        def epoch_update(carry, _):\n",
        "            \"\"\"Updates the parameters of the agent.\"\"\"\n",
        "\n",
        "            params, opt_state, batch = carry\n",
        "            grads = jax.grad(  # compute gradient on a single trajectory.\n",
        "                loss_fn)(params, batch)\n",
        "            grads = lax.pmean(grads, axis_name='j')  # reduce mean across cores.\n",
        "            grads = lax.pmean(grads, axis_name='i')  # reduce mean across batch.\n",
        "            updates, new_opt_state = opt_update(grads, opt_state)  # transform grads.\n",
        "            new_params = optax.apply_updates(params, updates)  # update parameters.\n",
        "            return (new_params, new_opt_state, batch), None\n",
        "\n",
        "        (new_params, new_opt_state, batch), _ = jax.lax.scan(epoch_update, (params_state.online, opt_state, batch), None, length=sgd_epochs)\n",
        "\n",
        "        new_params_state = Params(online=new_params, update_count=params_state.update_count+1)\n",
        "\n",
        "        return new_params_state, buffer_state, new_opt_state, rng, new_env_state, new_env_timestep\n",
        "\n",
        "    def learner_fn(params_state : Params, buffer_state, opt_state, rngs, env_states, env_timesteps):\n",
        "        \"\"\"Performs multiple updates of the agent.\"\"\"\n",
        "\n",
        "        batched_update_fn = jax.vmap(update_fn, axis_name='j')  # vectorize across batch.\n",
        "\n",
        "        def iterate_fn(_, val):  # repeat many times to avoid going back to Python.\n",
        "            params_state, buffer_state, opt_state, rngs, env_states, env_timesteps = val\n",
        "            return batched_update_fn(params_state, buffer_state, opt_state, rngs, env_states, env_timesteps)\n",
        "\n",
        "        return lax.fori_loop(0, iterations, iterate_fn, (\n",
        "            params_state, buffer_state, opt_state, rngs, env_states, env_timesteps))\n",
        "\n",
        "\n",
        "    return learner_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udGB9oghJTjt"
      },
      "source": [
        "### Create Experiment Fns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXTJZWA4JTjt"
      },
      "outputs": [],
      "source": [
        "def set_up_experiment(env, rollout_len, step_size, seed, buffer_size):\n",
        "    \"\"\"Sets up the experiment and returns the necessary information.\"\"\"\n",
        "\n",
        "    cores_count = len(jax.devices())  # get available TPU cores.\n",
        "    network = get_network_fn(env.action_spec.num_values)  # define network.\n",
        "    optim = optax.adam(step_size)  # define optimiser.\n",
        "\n",
        "    rng, rng_e, rng_p = random.split(random.PRNGKey(seed), num=3)  # prng keys.\n",
        "    _, timestep = env.reset(rng_e)\n",
        "    obs = timestep.observation.grid\n",
        "    dummy_obs = jnp.expand_dims(obs , 0) # dummy for net init.\n",
        "    params = network.init(rng_p, dummy_obs)  # initialise params.\n",
        "    opt_state = optim.init(params)  # initialise optimiser stats.\n",
        "    buffer_fn = fbx.make_trajectory_queue(\n",
        "        max_length_time_axis=buffer_size,\n",
        "        add_batch_size=1,\n",
        "        add_sequence_length=rollout_len,\n",
        "        sample_sequence_length=rollout_len\n",
        "    )\n",
        "    dummy_transition = TimeStep(observation=obs,\n",
        "                                action=jnp.zeros((), dtype=jnp.int32),\n",
        "                                reward=timestep.reward,\n",
        "                                discount=timestep.discount,\n",
        "                                behaviour_action_log_prob=jnp.zeros(()),\n",
        "                                behaviour_value=jnp.zeros(()),\n",
        "                                )\n",
        "    buffer_state = buffer_fn.init(dummy_transition) # initialise buffer.\n",
        "    return cores_count, network, optim, params, opt_state, buffer_fn, buffer_state, rng\n",
        "\n",
        "\n",
        "def get_rng_keys(cores_count, num_envs, rng):\n",
        "    \"\"\"Returns a list of rng keys for each environment.\"\"\"\n",
        "    rng, *step_rngs = jax.random.split(rng, cores_count * num_envs + 1)\n",
        "    reshape = lambda x: x.reshape((cores_count, num_envs) + x.shape[1:])\n",
        "    step_rngs = reshape(jnp.stack(step_rngs))  # add dimension to pmap over.\n",
        "    return rng, step_rngs\n",
        "\n",
        "def broadcast_to_device_shape(cores_count, num_envs, params, opt_state, buffer_state, rng):\n",
        "    \"\"\"Broadcasts the parameters to the shape of the device.\"\"\"\n",
        "    broadcast = lambda x: jnp.broadcast_to(x, (cores_count, num_envs) + x.shape)\n",
        "    params = jax.tree.map(broadcast, params)  # broadcast to cores and batch.\n",
        "    opt_state = jax.tree.map(broadcast, opt_state)  # broadcast to cores and batch\n",
        "    buffer_state = jax.tree.map(broadcast, buffer_state)  # broadcast to cores and batch\n",
        "    params_state = Params(online=params, update_count=jnp.zeros(shape=(cores_count, num_envs)))\n",
        "    rng, step_rngs = get_rng_keys(cores_count, num_envs, rng)\n",
        "    return params_state, opt_state, buffer_state, step_rngs, rng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tACRMGcEJTjt"
      },
      "source": [
        "### Create Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XU8CJbRJTjt"
      },
      "outputs": [],
      "source": [
        "env = jumanji.make(\"Snake-v1\", num_rows=6, num_cols=6)\n",
        "training_env = AutoResetWrapper(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8XS7iN9JTjt"
      },
      "source": [
        "### Define Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtMHrD4QJTjt"
      },
      "outputs": [],
      "source": [
        "# Number of Training-Evaluation iterations\n",
        "TRAINING_EVAL_ITERS = 20\n",
        "\n",
        "# Training parameters\n",
        "LEARNING_RATE = 3e-4\n",
        "SEED = 42\n",
        "NUM_ENVS = 8\n",
        "ROLLOUT_LEN = 64\n",
        "BUFFER_SIZE = ROLLOUT_LEN\n",
        "TRAINING_ITERS = 10\n",
        "AGENT_DISCOUNT = 0.99\n",
        "GAE_LAMBDA = 0.95\n",
        "CLIP_EPSILON = 0.2\n",
        "SGD_EPOCHS = 10\n",
        "\n",
        "# Evaluation parameters\n",
        "NUM_EVAL_EPISODES = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMoI9WtsJTjt"
      },
      "source": [
        "### Set Up Eval Fns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AKQ5NS_JTjt"
      },
      "outputs": [],
      "source": [
        "\n",
        "cores_count, network, optim, params, opt_state, buffer_fn, buffer_state, rng = set_up_experiment(\n",
        "    env=training_env,\n",
        "    rollout_len=ROLLOUT_LEN,\n",
        "    step_size=LEARNING_RATE,\n",
        "    seed=SEED,\n",
        "    buffer_size=BUFFER_SIZE,)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def eval_one_episode(params, rng):\n",
        "    \"\"\"Evaluates the agent on a single episode.\"\"\"\n",
        "\n",
        "    state, timestep = env.reset(rng)\n",
        "\n",
        "    def step(val):\n",
        "        params, state, timestep, tot_r, rng, done = val\n",
        "        rng, key_step = jax.random.split(rng)\n",
        "        obs = timestep.observation.grid\n",
        "        a_logits_t, v_t = network.apply(params, obs[jnp.newaxis,])\n",
        "        a_t = distrax.Categorical(a_logits_t).sample(seed=rng)[0]\n",
        "        state, timestep = env.step(state, a_t)\n",
        "        tot_r += timestep.reward\n",
        "        return (params, state, timestep, tot_r, rng, timestep.last())\n",
        "\n",
        "    params, state, timestep, tot_r, rng, done = jax.lax.while_loop(lambda val : val[5] == False, step, (params, state, timestep, 0, rng, False))\n",
        "\n",
        "    return params, tot_r\n",
        "\n",
        "@jax.jit\n",
        "def eval(params, rng):\n",
        "    \"\"\"Evaluates the agent on multiple episodes.\"\"\"\n",
        "\n",
        "    rngs = random.split(rng, NUM_EVAL_EPISODES)\n",
        "    params = jax.tree.map(lambda x: x[0][0], params)\n",
        "    _, tot_r = jax.lax.scan(eval_one_episode, params, rngs)\n",
        "    return tot_r.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx7_36XuJTjt"
      },
      "source": [
        "### Training the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hl_PMhkJTjt"
      },
      "outputs": [],
      "source": [
        "rng, *env_rngs = jax.random.split(rng, cores_count * NUM_ENVS + 1)\n",
        "env_states, env_timesteps = jax.vmap(env.reset)(jnp.stack(env_rngs))  # init envs.\n",
        "reshape = lambda x: x.reshape((cores_count, NUM_ENVS) + x.shape[1:])\n",
        "env_states = jax.tree.map(reshape, env_states)  # add dimension to pmap over.\n",
        "env_timesteps = jax.tree.map(reshape, env_timesteps)  # add dimension to pmap over.\n",
        "params_state, opt_state, buffer_state, step_rngs, rng = broadcast_to_device_shape(cores_count, NUM_ENVS, params, opt_state, buffer_state, rng)\n",
        "\n",
        "\n",
        "learn = get_learner_fn(env=training_env, forward_pass=network.apply, buffer_fn=buffer_fn, opt_update=optim.update, rollout_len=ROLLOUT_LEN, agent_discount=AGENT_DISCOUNT, iterations=TRAINING_ITERS, gae_lambda=GAE_LAMBDA, clip_epsilon=CLIP_EPSILON, sgd_epochs=SGD_EPOCHS)\n",
        "learn = jax.pmap(learn, axis_name='i')  # replicate over multiple cores.\n",
        "\n",
        "avg_reward = []\n",
        "total_time = 0\n",
        "for training_eval_iters in range(1, TRAINING_EVAL_ITERS+1):\n",
        "    # Train\n",
        "    start = timeit.default_timer()\n",
        "    params_state, buffer_state, opt_state, step_rngs, env_states, env_timesteps = learn(params_state, buffer_state, opt_state, step_rngs, env_states, env_timesteps)\n",
        "    params_state = jax.tree.map(lambda x: x.block_until_ready(), params_state) # wait for params to be ready so time is accurate.\n",
        "    total_time += timeit.default_timer() - start\n",
        "    # Eval\n",
        "    rng, eval_rng = jax.random.split(rng, num=2)\n",
        "    tot_r = eval(params_state.online, eval_rng)\n",
        "    avg_reward.append(tot_r)\n",
        "    if training_eval_iters % 2 == 0:\n",
        "        print(f\"Average Reward at iteration {training_eval_iters}: {tot_r}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2qQaKj_JTjt"
      },
      "source": [
        "### Plotting the reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0e8VxTIJTjt"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "num_env_transitions = ROLLOUT_LEN*TRAINING_ITERS*TRAINING_EVAL_ITERS\n",
        "x_values = np.asarray(jnp.arange(0, num_env_transitions, num_env_transitions/len(avg_reward)))\n",
        "avg_reward = np.asarray(avg_reward)\n",
        "\n",
        "sns.lineplot(x=x_values, y=avg_reward)\n",
        "plt.xlabel('Environment Transitions')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title(f'PPO on Snake-v1 - training time: {np.round((total_time)/60, 2)} minutes')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "v_flash",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}